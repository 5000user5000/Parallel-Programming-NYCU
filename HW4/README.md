# 作業 4 (HW4) Part 2: MPI 矩陣乘法筆記

這份文件整理了作業 4 Part 2 的核心目標、規則、挑戰以及建議的實作方法。

### 🎯 目標 (Goal)

- **最大化效能**：唯一的目標是盡可能地縮短矩陣乘法 `C = A * B` 的執行時間。

### 📜 規定與限制 (Rules & Constraints)

1.  **禁止 SIMD**：不允許使用任何 SIMD 指令集（如 SSE, AVX）來加速計算。
2.  **禁止多執行緒/GPU**：
    -   只能使用 MPI 進行平行化。
    -   不允許使用 OpenMP, Pthread, `fork()` 或任何 GPU 加速技術 (CUDA, OpenCL 等)。
    -   每個計算節點 (Node) 只能執行一個 MPI 行程 (Process)。
3.  **禁止第三方函式庫**：除了標準 C++ 函式庫和 MPI 函式庫外，禁止使用任何外部函式庫 (如 BLAS, LAPACK)。所有演算法必須自行實作。
4.  **禁止直接複製貼上**：可以參考網路上的演算法或程式碼，但必須親自重新撰寫，理解其運作原理。
5.  **矩陣維度**：`n`, `m`, `l` 的大小範圍可達 `1 <= n, m, l <= 10000`。
還有只能改動 matmul.cc

### ⚠️ 挑戰與注意事項 (Challenges & Key Points)

-   **記憶體限制**：當維度高達 10000 時，一個完整的 `10000x10000` 浮點數矩陣約佔 400MB。若演算法需要在每個節點都儲存整個矩陣 A 或 B，記憶體將會是主要瓶頸。
-   **通訊成本 (Communication Cost)**：MPI 的通訊操作（特別是 `MPI_Bcast` 一個大矩陣）非常耗時。高效的演算法必須最小化或分散化通訊成本。
-   **負載平衡 (Load Balancing)**：應確保每個 MPI 行程的計算負擔盡可能均勻，避免部分行程閒置等待。

### 🚀 極限效能攻略 (Ultimate Performance Guide)

要將 MPI 矩陣乘法的效能推到極限，我們不能只依賴單一技巧，而是要將多種策略結合，形成一個完整的、高度優化的系統。

#### 1. 核心演算法：選擇 SUMMA 而非 Cannon

雖然 2D 區塊分割是正確的方向，但在具體演算法選擇上，**SUMMA (Scalable Universal Matrix Multiplication Algorithm)** 通常比 Cannon's Algorithm 更具優勢。

-   **SUMMA 演算法概念**：
    1.  將 MPI 行程組織成 `P_r x P_c` 的二維網格。
    2.  每個行程 `P(i, j)` 負責計算 `C` 矩陣的 `C_ij` 區塊，並在本地初始化為 0。
    3.  演算法會迭代 `m / b` 次（`b` 是區塊大小）。在第 `k` 次迭代中：
        *   擁有 `A_ik` 區塊的行程（通常是網格第 `i` 列的行程）會將其**沿著網格的「列」廣播**給所有 `P(i, *)` 的行程。
        *   擁有 `B_kj` 區塊的行程（通常是網格第 `k` 行的行程）會將其**沿著網格的「行」廣播**給所有 `P(*, j)` 的行程。
        *   每個行程 `P(i, j)` 收到廣播來的 `A_ik` 和 `B_kj` 區塊後，在本地進行矩陣乘法，並將結果累加到自己的 `C_ij` 區塊中：`C_ij += A_ik * B_kj`。
-   **為什麼 SUMMA 更好？**
    *   **彈性**：對處理器網格的維度沒有特殊要求（不需為方陣），對矩陣維度也更具彈性。
    *   **效率**：通訊模式（沿著行/列廣播）通常能被 MPI 高效實現，且易於和計算重疊。

#### 2. 本地計算優化：榨乾單核心效能 (無 SIMD)

在每個 MPI 行程內部，本地的矩陣乘法 `C_local += A_local * B_local` 必須極致優化。

-   **三層快取區塊化 (Three-Level Cache Blocking)**：
    *   **L3 Cache Block**: 最大的區塊，確保參與計算的 A, B, C 區塊能完全放入 L3 快取。
    *   **L2 Cache Block**: 在 L3 區塊內再切分，確保當前運算的 A, B 區塊能放入 L2 快取。
    *   **L1 Cache Block**: 在 L2 區塊內再切分，確保最內層迴圈的資料能放入 L1 快取。
    *   這需要精心挑選區塊大小 (`BLOCK_K`, `BLOCK_J`, `BLOCK_I`)，通常需要根據目標機器的快取大小進行調整。

-   **暫存器區塊化 (Register Blocking / Micro-kernel)**：
    *   這是最內層的計算核心。目標是將 C 區塊的一個極小部分（如 `4x4` 或 `8x4`）的累加值完全保存在 CPU 暫存器中。
    *   透過**手動展開迴圈 (Manual Loop Unrolling)**，可以最大化資料在暫存器中的重複使用，並減少迴圈的額外開銷。

-   **指標混疊與預取 (Pointer Aliasing & Prefetching)**：
    *   使用 `__restrict__` 關鍵字告訴編譯器指標不會指向重疊的記憶體區域，允許其進行更激進的優化。
    *   使用 `__builtin_prefetch` 在資料被實際使用前，提前將其從主記憶體載入快取，以隱藏記憶體延遲。

#### 3. MPI 通訊優化：重疊計算與通訊

這是 MPI 效能的精髓所在：**永遠不要讓 CPU 閒著等待網路**。

-   **非阻塞通訊 (Non-Blocking Communication)**：
    *   使用 `MPI_Ibcast` (或 `MPI_Isend`/`MPI_Irecv`) 來啟動通訊，然後**立即**開始處理**上一步**已收到的資料。
    *   **軟體管線化 (Software Pipelining)**：
        1.  **預取 (Pre-fetch)**：啟動下一個 `A` 和 `B` 區塊的非阻塞接收 (`MPI_Irecv`)。
        2.  **計算 (Compute)**：對當前已經擁有的 `A` 和 `B` 區塊執行本地矩陣乘法。
        3.  **等待 (Wait)**：使用 `MPI_Wait` 確保預取的資料已經接收完畢。
        4.  這個模式將通訊延遲隱藏在計算時間之後，是效能的關鍵。

-   **MPI 衍生資料型態 (Derived Datatypes)**：
    *   在分發和收集非連續的矩陣區塊時（例如，從 `rank 0` 的大矩陣中收集所有 `C_ij` 區塊），使用 `MPI_Type_vector` 或 `MPI_Type_create_subarray` 建立資料型態。
    *   這避免了手動將資料打包 (pack) 到連續緩衝區再傳送的開銷，MPI 函式庫可以直接存取非連續的記憶體，效率更高。

#### 4. 適應性策略 (Adaptive Strategy)

針對兩個不同規模的資料集，我們可以採用不同的策略。

-   **混合演算法 (Hybrid Algorithm)**：
    *   **對於 Set 1 (300-500)**：矩陣較小，通訊建立的成本可能佔比較高。一個高度優化的 **1D 分割演算法**可能因為其簡單的通訊模式（一次 Scatterv，一次 Bcast）反而比複雜的 2D SUMMA 演算法更快。
    *   **對於 Set 2 (1000-2000)**：矩陣巨大，記憶體和通訊頻寬是瓶頸。**2D SUMMA 演算法**的記憶體和通訊擴展性優勢將會完全體現。
    *   可以在程式中加入一個判斷式 `if (n < THRESHOLD) { ... // 1D logic } else { ... // 2D logic }`。

-   **動態參數調整 (Dynamic Parameter Tuning)**：
    *   **處理器網格維度**：`P_r` 和 `P_c` 的比例會影響通訊效能。通常，讓網格更接近方形（`P_r ≈ P_c`）是一個好的起點，但最佳比例可能取決於 `n`, `m`, `l` 的相對大小。
    *   **區塊大小**：本地計算的快取區塊大小應根據矩陣維度和快取大小來決定，而不是寫死。

### 📊 測試資料集 (Data Sets)

作業將使用兩個資料集進行測試，你可以在 `/data/pp/hw4/test` 路徑下找到它們。針對不同的資料集，你可能需要調整實作策略以達到最佳效能。

| Set | Credit | `n, m, l` 範圍 | 執行指令 (`-n` 為行程數) |
| :-- | :----- | :------------- | :----------------------- |
| 1   | 10     | 300-500        | `mpirun -n 4 ./matmul <data_file>` |
| 2   | 10     | 1000-2000      | `mpirun -n 5 ./matmul <data_file>` |

### 🛠️ 測試環境 (Testing Environment)

了解目標硬體對於極致效能優化至關重要。

-   **OS**: Debian 12.9
-   **Compiler**: g++-12
-   **CPU**: Intel(R) Core(TM) i5-10500 @ 3.10GHz

#### CPU 快取架構 (Cache Hierarchy)

這是進行本地計算優化 (Cache Blocking) 的關鍵依據：

-   **L1 Data Cache**: `32 KB` (per core)
-   **L2 Cache**: `256 KB` (per core)
-   **L3 Cache**: `12 MB` (shared)

我們的本地矩陣乘法核心將根據這些快取大小來精心設計區塊尺寸，以最大化資料重用並最小化記憶體延遲。